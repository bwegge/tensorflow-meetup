{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendations with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to implement a factorization machine and train it on movie ratings.  \n",
    "The code is partly taken from [Factorization Machines with Tensorflow](http://nowave.it/factorization-machines-with-tensorflow.html) by Gabriele Modena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(\"../../scripts\")\n",
    "sys.path.append(module_path)\n",
    "import recommendation_helper as rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 99\n",
    "training_epochs = 200\n",
    "k = 5\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Constants which control the impact of the regularization terms\n",
    "lambda_w = tf.constant(0.001, name='lambda_w')\n",
    "lambda_V = tf.constant(0.001, name='lambda_V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignored_columns = [\"movieId\", \"userId\", \"title\", \"year_bucket\", \"rating\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and generate a train and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Loads data and splits into train and test set.\"\"\"\n",
    "    data = pd.read_csv(tf.gfile.Open(\"../../data/05/movielens.csv\"), sep=\";\")\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    msk = np.random.rand(len(data)) < 0.9     # split into 90% train and 10% test data\n",
    "    return data.loc[msk], data.loc[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns which are not used for training, e.g. movie and user ID which are only used for generating recommendations later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_data(input):\n",
    "    return input.drop([col for col in input.columns if col.startswith(tuple(ignored_columns))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = load_data()\n",
    "print(\"Set contains %i training samples and %i test samples.\" % (len(df_train), len(df_test)))\n",
    "\n",
    "train = get_input_data(df_train)\n",
    "test = get_input_data(df_test)\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the placeholders and variables needed for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, p = train.shape\n",
    "\n",
    "X = tf.placeholder('float', shape=[None, p], name=\"X\")\n",
    "y = tf.placeholder('float', shape=[None, 1], name=\"y\")\n",
    "\n",
    "# interaction factors, randomly initialized\n",
    "V = tf.Variable(tf.random_normal([k, p], stddev=0.01), name=\"V\")\n",
    "\n",
    "# bias and weights\n",
    "w0 = tf.Variable(tf.zeros([1]), name=\"w0\")\n",
    "w = tf.Variable(tf.zeros([p]), name=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factorization machine computes the following function:  \n",
    "$$\\hat{y}(x) = w_0 + \\sum_{i=1}^{p} w_i \\cdot X_i + \\frac{1}{2} \\cdot \\sum_{i=1}^{k} (X_k V^T_k)^2 - (X_k^2 (V_k^2)^T)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # linear_terms = w0 + Σ_{i=1}^{p} w_i * X_i\n",
    "    linear_terms = tf.add(w0, tf.reduce_sum(tf.multiply(w, X), 1, keep_dims=True))\n",
    "    \n",
    "    # interactions = 0.5 * Σ_{i=1}^{k} (X_k * V^T_k)^2 - (X_k^2 * (V_k^2)^T)\n",
    "    interactions = tf.multiply(0.5, tf.reduce_sum(\n",
    "                       tf.subtract(tf.square(tf.matmul(X, tf.transpose(V))),\n",
    "                                             tf.matmul(tf.square(X), tf.transpose(tf.square(V)))),\n",
    "                       1, keep_dims=True))\n",
    "    \n",
    "    # Final step: add linear and interaction terms\n",
    "    y_hat = tf.add(linear_terms, interactions, name=\"prediction\")\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We optimize the model using the Mean Squared Error with L2-regularization over the weights and latent factors as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_optimizer(y_hat):\n",
    "    # Mean Squared Error: 1/n Σ(y - y_hat)^2\n",
    "    mse = tf.reduce_mean(tf.square(tf.subtract(y, y_hat)))\n",
    "    \n",
    "    # L2 regularization over W and V: Σ(λ_w * W^2 + λ_V * V^2)\n",
    "    l2_norm = tf.reduce_sum(tf.add(tf.multiply(lambda_w, tf.square(w)),\n",
    "                                   tf.multiply(lambda_V, tf.square(V))))\n",
    "\n",
    "    # Total loss used for optimization\n",
    "    loss = tf.add(mse, l2_norm)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return optimizer, mse, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_metrics(y_hat):\n",
    "    # Mean Absolute Error |y - y_hat|\n",
    "    mae = tf.reduce_mean(tf.abs(y - y_hat))\n",
    "    \n",
    "    # Root Mean Squared Error √(1/n Σ(y - y_hat)^2)\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y, y_hat))))\n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    # Set up the model, optimizer and metrics\n",
    "    y_hat = build_model()\n",
    "    optimizer, mse, loss = create_optimizer(y_hat)\n",
    "    mae, rmse = create_metrics(y_hat)\n",
    "\n",
    "    # IMPORTANT: initialize variables before computing anything else\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(training_epochs):\n",
    "        # Shuffle the training data\n",
    "        indices = np.array(train.index)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # Train the model\n",
    "        x_data, y_data = train.loc[indices], pd.DataFrame({\"rating\": df_train.rating.loc[indices]})\n",
    "        sess.run(optimizer, feed_dict={X: x_data, y: y_data})\n",
    "        if epoch % 10 == 9:\n",
    "            error, cost, mean_abs, root_mean = sess.run([mse, loss, mae, rmse], feed_dict={X: x_data, y: y_data})\n",
    "            print(\"Epoch %i\\tLoss (regularized error): %.3f\\tMSE: %.3f\\tRMSE: %.3f\\tMAE: %.3f\" \n",
    "                  % ((epoch + 1), cost, error, root_mean, mean_abs))\n",
    "\n",
    "    print('MSE Test:', sess.run(mse, feed_dict={X: test, y: pd.DataFrame({\"rating\": df_test.rating})}))\n",
    "    print('RMSE Test:', sess.run(rmse, feed_dict={X: test, y: pd.DataFrame({\"rating\": df_test.rating})}))\n",
    "    print('MAE Test:', sess.run(mae, feed_dict={X: test, y: pd.DataFrame({\"rating\": df_test.rating})}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our model, we can use it to recommend movies to users.\n",
    "\n",
    "For this, we need the nodes in the session graph which take the input and compute the estimates for the user's ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the necessary nodes from the model graph\n",
    "prediction = sess.graph.get_tensor_by_name(\"prediction:0\")\n",
    "X = sess.graph.get_tensor_by_name(\"X:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the first *n* recommendations for a user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_n_recommendations(user_id, n):\n",
    "    input = rh.get_user_data(user_id)\n",
    "    recommendations = rh.movies[[\"title\", \"movieId\"]]\n",
    "    \n",
    "    # Here, we use the nodes which we extracted before\n",
    "    recommendations[\"rating\"] = sess.run(prediction, feed_dict={X: input})\n",
    "    return recommendations[[\"title\", \"movieId\", \"rating\"]].sort_values(\"rating\", ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_n_recommendations(1, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Optional tasks**:\n",
    "* Experiment with the model parameters. Try, e.g., different values for *k* and different learning rates.\n",
    "* Try training the model with no/less/more regularization. (Hint: vary the values for $\\lambda_V$ and $\\lambda_w$).\n",
    "* Try different optimization algorithms (see [TensorFlow Optimizers](https://www.tensorflow.org/api_guides/python/train#Optimizers) for an overview of optimizers).\n",
    "* Compare the recommendations for different users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
