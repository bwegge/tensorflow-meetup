{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendations with TensorFlow Wide & Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to implement the Wide & Deep Model and train it on movie ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(\"../scripts\")\n",
    "sys.path.append(module_path)\n",
    "import recommendation_helper as rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 99\n",
    "seed_init = 42\n",
    "training_epochs = 70\n",
    "learning_rate = 0.005\n",
    "beta = 0.0\n",
    "batch_size = 128\n",
    "n_hidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_wide_columns = [\"movieId\", \"userId\", \"title\", \"year\", \"rating\"]\n",
    "not_deep_columns = [\"movieId\", \"userId\", \"title\", \"year_bucket\", \"rating\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and generate a train and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Loads data and splits into train and test set.\"\"\"\n",
    "    data = pd.read_csv(tf.gfile.Open(\"../data/05/movielens.csv\"), sep=\";\")\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    msk = np.random.rand(len(data)) < 0.9     # split into 90% train and 10% test data\n",
    "    return data.loc[msk], data.loc[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns which are not used for training, e.g. movie and user ID which are only used for generating recommendations later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_data(input, type):\n",
    "    if type == \"wide\":\n",
    "        return input.drop(not_wide_columns, axis=1)\n",
    "    elif type == \"deep\":\n",
    "        drop_cols = [col for col in input.columns if col.startswith(tuple(not_deep_columns))]\n",
    "        return input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = load_data()\n",
    "print(\"Set contains %i training samples and %i test samples.\" % (len(df_train), len(df_test)))\n",
    "\n",
    "# Input\n",
    "train_w = get_input_data(df_train, \"wide\")\n",
    "test_w = get_input_data(df_test, \"wide\")\n",
    "train_d = get_input_data(df_train, \"deep\")\n",
    "test_d = get_input_data(df_test, \"deep\")\n",
    "train_y = pd.DataFrame({\"rating\": df_train.rating})\n",
    "test_y = pd.DataFrame({\"rating\": df_test.rating})\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the placeholders and variables needed for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for initializing variables like weights and biases\n",
    "def compute_stddev(x):\n",
    "    return tf.to_float(tf.sqrt(2 / tf.reduce_prod(x.get_shape()[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, n_w = train_w.shape\n",
    "_, n_d = train_d.shape\n",
    "\n",
    "x_w = tf.placeholder(\"float\", [None, n_w], name=\"x_w\")\n",
    "x_d = tf.placeholder(\"float\", [None, n_d], name=\"x_d\")\n",
    "y = tf.placeholder(\"float\", [None, 1], name=\"y\")\n",
    "keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "\n",
    "# Trainable model parameters\n",
    "weights = {\n",
    "    \"linear\": tf.Variable(tf.truncated_normal([n_w, 1], seed=seed_init, stddev=compute_stddev(x_w)), name=\"weights_linear\"),\n",
    "}\n",
    "biases = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wide & Deep Model combines a linear model and a deep neural network.\n",
    "\n",
    "**Task**: Implement the Wide & Deep Model.\n",
    "* Create the linear component of the model by multiplying the input *x_w* with the weights for the linear layer, which we already defined for you.\n",
    "\n",
    "\n",
    "* Create one hidden layer in the deep component\n",
    "    * Create weights with shape [*n_d, n_hidden*] and biases with shape [*n_hidden*].\n",
    "    * Both the weights and the biases are initialized with a truncated normal distribution which uses *seed_init* as seed and a standard deviation depending on *x_d*. (Hint: *compute_stddev(x)* above)\n",
    "    * Multiply the input *x_d* and the weights. Add the biases to the matrix product.\n",
    "    * Apply dropout and the relu activation function. \n",
    "    \n",
    "    \n",
    "* Create the output layer of the deep component\n",
    "    * Create weights with shape [*n_hidden*, 1] which are initialized like the weights above. The standard deviation depends on the activations of the hidden layer.\n",
    "    * Multiply the hidden layer and the new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(keep_prob):\n",
    "    ## YOUR CODE HERE\n",
    "    # Linear layer with linear activation\n",
    "    linear = \n",
    "    \n",
    "    # Hidden layer with relu activation\n",
    "    \n",
    "    # Deep output layer\n",
    "    deep =\n",
    "\n",
    "    ##\n",
    "    # Output layer with linear activation\n",
    "    biases[\"out\"] = tf.Variable(tf.truncated_normal([1], seed=seed_init, \n",
    "                                                    stddev=compute_stddev(deep)), name=\"biases_out\")\n",
    "    out_layer = tf.add(tf.add(linear, deep), biases['out'], name=\"prediction\")\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We optimize the model using the Mean Squared Error with L2-regularization over the weights as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_optimizer(y_hat):\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Define loss (MSE) and optimizer\n",
    "    mse = tf.reduce_mean(tf.square(tf.subtract(y, y_hat)))\n",
    "    \n",
    "    # L2 regularization over weights\n",
    "    regularizer = tf.nn.l2_loss(weights[\"linear\"]) + tf.nn.l2_loss(weights[\"h1\"]) + \\\n",
    "                  tf.nn.l2_loss(weights[\"deep\"])\n",
    "    loss = mse + beta * regularizer\n",
    "    \n",
    "    # Learning rate which is exponentially decayed, so first optimization steps are larger\n",
    "    lr = tf.train.exponential_decay(learning_rate, global_step, 10, 0.9, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "    return optimizer, mse, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_metrics(y_hat):\n",
    "    # Mean Absolute Error |y - y_hat|\n",
    "    mae = tf.reduce_mean(tf.abs(y - y_hat))\n",
    "    \n",
    "    # Root Mean Squared Error √(1/n Σ(y - y_hat)^2)\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y, y_hat))))\n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(kp=1.0):\n",
    "    # Set up the model, optimizer and metrics\n",
    "    y_hat = build_model(kp)\n",
    "    optimizer, mse, loss = create_optimizer(y_hat)\n",
    "    mae, rmse = create_metrics(y_hat)\n",
    "\n",
    "    # IMPORTANT: initialize variables before computing anything else\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch = int(df_train.shape[0] / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x_w = train_w.sample(batch_size)\n",
    "            batch_x_d = train_d.loc[batch_x_w.index]\n",
    "            batch_y = pd.DataFrame({\"rating\": df_train.rating.loc[batch_x_w.index]})\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l, = sess.run([optimizer, loss], feed_dict={x_w: batch_x_w, x_d: batch_x_d, y: batch_y, keep_prob: kp})\n",
    "\n",
    "            # Compute average loss and training accuracy\n",
    "            avg_loss += l / total_batch\n",
    "\n",
    "        if epoch % 5 == 4:\n",
    "            error, cost, mean_abs, root_mean = sess.run([mse, loss, mae, rmse], \n",
    "                                                        feed_dict={x_w: train_w, x_d: train_d, y: train_y})\n",
    "            print(\"Epoch %i\\tLoss (regularized error): %.3f\\tMSE: %.3f\\tRMSE: %.3f\\tMAE: %.3f\" \n",
    "                  % ((epoch + 1), cost, error, root_mean, mean_abs))\n",
    "    \n",
    "    print('MSE Test:', sess.run(mse, feed_dict={x_w: test_w, x_d: test_d, y: test_y}))\n",
    "    print('RMSE Test:', sess.run(rmse, feed_dict={x_w: test_w, x_d: test_d, y: test_y}))\n",
    "    print('MAE Test:', sess.run(mae, feed_dict={x_w: test_w, x_d: test_d, y: test_y}))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(kp=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our model, we can use it to recommend movies to users.\n",
    "\n",
    "For this, we need the nodes in the session graph which take the input and compute the estimates for the user's ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the necessary nodes from the model graph\n",
    "prediction = sess.graph.get_tensor_by_name(\"prediction:0\")\n",
    "x_w = sess.graph.get_tensor_by_name(\"x_w:0\")\n",
    "x_d = sess.graph.get_tensor_by_name(\"x_d:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the first *n* recommendations for a user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_n_recommendations(user_id, n):\n",
    "    input_w, input_d = rh.get_user_data(user_id, \"wide_n_deep\")\n",
    "    recommendations = rh.movies[[\"title\", \"movieId\"]]\n",
    "    \n",
    "    # Here, we use the nodes which we extracted before\n",
    "    recommendations[\"rating\"] = sess.run(prediction, feed_dict={x_w: input_w, x_d: input_d})\n",
    "    return recommendations[[\"title\", \"movieId\", \"rating\"]].sort_values(\"rating\", ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_n_recommendations(1, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional tasks**:\n",
    "* Experiment with the model parameters. Try, e.g., different learning rates or hidden layer sizes.\n",
    "* Add L2-regularization or vary the dropout rate.\n",
    "* Use several hidden layers. We implemented the function *create_relu_layer(n_this, n_previous, x, keep_prob, name)* for faster creation of layers with RELU activation.\n",
    "* Try different optimization algorithms or activation functions (see [TensorFlow Activation Functions](https://www.tensorflow.org/api_guides/python/nn#Activation_Functions)).\n",
    "* Compare the recommendations for different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example usage: hidden = rh.create_relu_layer(n_hidden, n_d, x_d, keep_prob, \"h1\")\n",
    "def create_relu_layer(n_this, n_previous, x, keep_prob, name):\n",
    "    weights[name] = tf.Variable(tf.truncated_normal([n_previous, n_this], seed=seed_init, stddev=compute_stddev(x)), name=\"weights_\" + name)\n",
    "    biases[name] = tf.Variable(tf.truncated_normal([n_this], seed=seed_init, stddev=compute_stddev(x)), name=\"biases_\" + name)\n",
    "\n",
    "    layer = tf.add(tf.matmul(x, weights[name]), biases[name])\n",
    "    layer = tf.nn.relu(tf.nn.dropout(layer, keep_prob))\n",
    "    return layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
